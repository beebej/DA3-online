{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eff5df-e219-4dc8-8a22-40ce22763213",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run the following code to print multiple outputs from a cell\n",
    "get_ipython().ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0fa0c-294a-483a-b6e5-f82f1ba0e822",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Importing & Setting up the Data\n",
    "Import the file, \"creditCardDefaultReduced.csv\", and save it in a variable called `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f3516-7be8-4b4e-bf29-f4804f38465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"creditCardDefaultReduced.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf9bf0-1119-4242-a042-370fbae72ad7",
   "metadata": {},
   "source": [
    "### Outcome Variable\n",
    "Next, create your `outcome` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814c9a4-bc9d-4489-90f6-8ad685dcc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = df[\"Payment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a300b91-320b-4f19-a9f2-37f7e18e337d",
   "metadata": {},
   "source": [
    "### Features Variable\n",
    "For the `features` variable, first save the numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca4b1f-b05a-4e54-be9b-3442951f7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericFeatures = df[[\"Limit_Bal\", \"Bill_Amt1\", \"Pay_Amt1\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e0826-4bd7-4a8b-8310-dac207b97320",
   "metadata": {},
   "source": [
    "Next, create dummy variables for your categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194ad6e-8b3b-42fa-805c-d09b6eb9470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummiesMarriage = pd.get_dummies(df[\"Marriage\"], prefix = \"Marriage\", drop_first = True)\n",
    "dummiesCard = pd.get_dummies(df[\"Card\"], prefix = \"Card\", drop_first = True)\n",
    "dummiesPay_0 = pd.get_dummies(df[\"Pay_0\"], prefix = \"Pay_0\", drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6256a-9316-4349-9b73-fd6bf7ed106a",
   "metadata": {},
   "source": [
    "Now combine the numeric features and dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa886e97-a08c-4600-a464-458f55d5e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([numericFeatures, dummiesMarriage, dummiesPay_0, dummiesCard], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcb0fa-010c-4833-9c5e-9170e317961a",
   "metadata": {},
   "source": [
    "### Partition the Data\n",
    "\n",
    "Let's partition the data into training and test data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cea9ba-6f8a-4a74-8e2d-2c8d3409a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "featuresTrain, featuresTest, outcomeTrain, outcomeTest = train_test_split(features, \n",
    "                                                                          outcome, \n",
    "                                                                          test_size = 0.33, \n",
    "                                                                          random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1df7aa-c076-418b-8a61-68eee8b6e4fe",
   "metadata": {},
   "source": [
    "### Scale the Features\n",
    "\n",
    "Similar to the Support Vector Machine model, it's a good idea to scale your features with Neural Network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f96f58-8000-45b5-a5fd-2d398ecd8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "featuresTrain_norm = scaler.fit_transform(featuresTrain) #you fit to the training features\n",
    "featuresTest_norm = scaler.transform(featuresTest)       #you only transform the test features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfd480-837d-4ed4-9963-8bf44fa955aa",
   "metadata": {},
   "source": [
    "## Building a Neural Network Model\n",
    "\n",
    "Remember, there was a 4 step process for building our models last class:\n",
    "1. Set-up the model\n",
    "2. Fit the training data to the model\n",
    "3. Predict outcomes using the model\n",
    "4. Assess the fit of the model\n",
    "\n",
    "Refer to the code from last class to build a neural network model. Here are several pieces of information you'll need:\n",
    "\n",
    "* The package is `sklearn.neural_network`\n",
    "* The function we'll be using is `MLPClassifier()`\n",
    "    - Set the `random_state` to 42\n",
    "    - You'll also need to specify `hidden_layer_sizes = (30,30)` inside the `MLPClassifier()` function\n",
    "* Get the classification report for both the training and test predictions\n",
    "* Get the confusion matrix for the test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982034b3-ac4e-4613-b514-52fb8829322e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db7113dc-1509-4e2d-80d6-59f196d9ca56",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) models are models designed to process human language for a variety of tasks, such as sentiment analysis, summarizing text, translating text, and -- as we've seen with ChatGPT -- answer questions.\n",
    "\n",
    "ChatGPT is a large language model that uses a version of neural network deep learning called Generative Pre-trained Transformers (GPT) that are trained on 175 billion parameters that allow the model to weigh the importance of different words relative to each other.\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "To demonstrate how NLP models work (at a level that our laptops can handle), let's build a sentiment analysis model that can classify whether a film review is positive or negative.\n",
    "\n",
    "First, let's import our training dataset of 8,957 reviews from IMDb.com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbadb4-e7b0-4d73-b928-98aaf4ef6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\"IMDB_reduced.xlsx\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda73aec-d5a2-4e8d-b1a4-92e4702c7da7",
   "metadata": {},
   "source": [
    "For this model, our `outcome` variable is `label` (0 = negative review, 1 = positive review) and our `feature` is the review `text`. In the next code cell, create an `outcomeIMDB` and `featureIMDB` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d7879-b75a-4e54-8d06-84a950b0cc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5202b19-54a1-4885-bb01-c431a9de86d4",
   "metadata": {},
   "source": [
    "Now partition the data into training and test data sets, using `random_state = 42` and a `test_size` of 33%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd1aad-3ac9-414f-915c-baa4c6332b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b894e35-1adb-4d23-b19a-243d708859ed",
   "metadata": {},
   "source": [
    "### Vectorizing the Text\n",
    "\n",
    "Now, we need to convert our review text into numeric data. To do this, we'll use the `TfidfVectorizer()`. Similar to the scaler we used above, we first need to initialize the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be10218-8039-46dd-a9bc-260ffbf6eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 500, tokenizer = casual_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b3ed6-b5ed-49ef-8a0a-5f0def3a1503",
   "metadata": {},
   "source": [
    "The Term Frequency (TF) and Inverse Document Frequency (IDF) score weights each word in a review based on how many times it appears in that review and how unique or important it is across all reviews in the training data set. A higher number means it's more important.\n",
    "\n",
    "The tokenizer we are using (`casual_tokenize`) splits the review into individual words and can handle punctuation and special characters. We are setting `max_features` to only tokenize the top 500 most important words or characters. \n",
    "\n",
    "Next, we transform our training and test features using the vectorizer, again similar to the scaler we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482a53f-f003-40fa-97ad-aaed2d5251ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIMDBTrain_v = vectorizer.fit_transform(featureIMDBTrain)\n",
    "featureIMDBTest_v = vectorizer.transform(featureIMDBTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d130f-85f6-442f-a7e6-472746ea674f",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n",
    "Now we can build a neural network model using these vectorized features. Copy/paste your neural network code from above and replace the features and outcome variables as needed. Also use the following settings:\n",
    "\n",
    "* Set the `random_state` to 42\n",
    "* Use (10,10,10) for the `hidden_layer_sizes`\n",
    "* Name your model `modReview`\n",
    "\n",
    "Be sure to print out the classification reports and the confusion matrix for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab817d86-f846-4371-8a0f-1a34e23e2ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e3ddde8-2ca2-4308-b099-40a98c9d24b4",
   "metadata": {},
   "source": [
    "### Using the Model\n",
    "\n",
    "Now that we've built our model, we can use it to classify new film reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c2f95-4f25-44a5-9160-21b0e5dce01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New film review\n",
    "new_review = [\"I really enjoyed the film. It kept me entertained the whole time.\"]\n",
    "\n",
    "# Tokenize and vectorize the new review using the same vectorizer\n",
    "new_review_v = vectorizer.transform(new_review)\n",
    "\n",
    "# Predict sentiment on the new review\n",
    "modReview.predict(new_review_v)          # predicts 0 or 1\n",
    "modReview.predict_proba(new_review_v)    # provides probability of 0 or 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
