{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eff5df-e219-4dc8-8a22-40ce22763213",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run the following code to print multiple outputs from a cell\n",
    "get_ipython().ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0fa0c-294a-483a-b6e5-f82f1ba0e822",
   "metadata": {},
   "source": [
    "# Predictive Performance\n",
    "\n",
    "Over the past several classes, we've built a number of classification models that we assessed using accuracy. For example, when predicting whether or not someone paid their credit card bill, we got the following results:\n",
    "\n",
    "* Decision Tree: 71% accurate\n",
    "* Random Forest: 80% accurate\n",
    "* Support Vector Machine (w/o normalized features): 78%\n",
    "* Support Vector Machine (w/ normalized features): 82% accurate\n",
    "* Neural Network: 82% accurate\n",
    "\n",
    "The major drawback to using this number -- as we've seen -- is the fact that it doesn't take into account how accurate (or inaccurate) the model is for paid vs. missed payments. In our example, it's more costly for the bank to misclassify someone as paying their bill when they didn't pay. However, the SVM model without normalized features predicted *no one* would miss their bill and yet still achieved 78% accuracy. This often occurs with imbalanced data, like we had with the credit card customers.\n",
    "\n",
    "Two measures that try to account for this are the ROC (receiver operating characteristic) curve and the AUC (area under the curve).\n",
    "\n",
    "## Importing & Setting up the Data\n",
    "Import the file, \"creditCardDefaultReduced.csv\", and save it in a variable called `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f3516-7be8-4b4e-bf29-f4804f38465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"creditCardDefaultReduced.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf9bf0-1119-4242-a042-370fbae72ad7",
   "metadata": {},
   "source": [
    "Next, create your `outcome` and `features` variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814c9a4-bc9d-4489-90f6-8ad685dcc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = df[\"Payment\"]\n",
    "numericFeatures = df[[\"Limit_Bal\", \"Bill_Amt1\", \"Pay_Amt1\", \"Age\"]]\n",
    "dummiesMarriage = pd.get_dummies(df[\"Marriage\"], prefix = \"Marriage\", drop_first = True)\n",
    "dummiesCard = pd.get_dummies(df[\"Card\"], prefix = \"Card\", drop_first = True)\n",
    "dummiesPay_0 = pd.get_dummies(df[\"Pay_0\"], prefix = \"Pay_0\", drop_first = True)\n",
    "features = pd.concat([numericFeatures, dummiesMarriage, dummiesPay_0, dummiesCard], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcb0fa-010c-4833-9c5e-9170e317961a",
   "metadata": {},
   "source": [
    "Our next step is to partition the data into training and test data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cea9ba-6f8a-4a74-8e2d-2c8d3409a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "featuresTrain, featuresTest, outcomeTrain, outcomeTest = train_test_split(features, \n",
    "                                                                          outcome, \n",
    "                                                                          test_size = 0.33, \n",
    "                                                                          random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1df7aa-c076-418b-8a61-68eee8b6e4fe",
   "metadata": {},
   "source": [
    "Let's scale our features and use these features for all of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f96f58-8000-45b5-a5fd-2d398ecd8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "featuresTrain_norm = scaler.fit_transform(featuresTrain)\n",
    "featuresTest_norm = scaler.transform(featuresTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfd480-837d-4ed4-9963-8bf44fa955aa",
   "metadata": {},
   "source": [
    "## Building the Models\n",
    "\n",
    "The following code cell builds our 4 models...we will be using these to assess the models' fit using ROC curves and the AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982034b3-ac4e-4613-b514-52fb8829322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "import sklearn.tree\n",
    "modTree = sklearn.tree.DecisionTreeClassifier(random_state = 42)\n",
    "modTree.fit(featuresTrain_norm, outcomeTrain)\n",
    "\n",
    "# Random Forest\n",
    "import sklearn.ensemble\n",
    "modForest = sklearn.ensemble.RandomForestClassifier(random_state = 42)\n",
    "modForest.fit(featuresTrain_norm, outcomeTrain)\n",
    "\n",
    "# Support Vector Machine\n",
    "import sklearn.svm\n",
    "modSVM = sklearn.svm.SVC(random_state = 42)\n",
    "modSVM.fit(featuresTrain_norm, outcomeTrain)\n",
    "\n",
    "# Neural Network\n",
    "import sklearn.neural_network\n",
    "modNN = sklearn.neural_network.MLPClassifier(random_state = 42, hidden_layer_sizes = (30,30))\n",
    "modNN.fit(featuresTrain_norm, outcomeTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7113dc-1509-4e2d-80d6-59f196d9ca56",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "Another limitation of the accuracy score is that it only provides a measure of the model's performance at a 50% threshold...i.e., the credit card customer is characterized as missing the payment unless the probability of payment exceeds 50%. There are, of course, other thresholds the bank could consider. For example, if it's costly to misclassify customers who will miss their payment, maybe the bank should use a threshold of 60% instead.\n",
    "\n",
    "The ROC curve plots a model's true positive rate against its false positive rate at different thresholds (from 0% to 100%). We'll be using the `roc_curve()` function from `sklearn.metrics`, which returns an array of false positive rates, true positive rates, and the thresholds at which those rates were calculated.\n",
    "\n",
    "Let's see how this works, starting with the Decision Tree (`modTree`). We first need to get predictions from our model so that we can compare them to the actual outcomes. Up until now, we've calculated the predictions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbadb4-e7b0-4d73-b928-98aaf4ef6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "modTree.predict(featuresTest_norm)[0:19]\n",
    "# [0:20] displays the first 19 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda73aec-d5a2-4e8d-b1a4-92e4702c7da7",
   "metadata": {},
   "source": [
    "Notice the predictions are either `Paid` or `Missed`. But we want to be able to use probabilities in our calculations. So let's use the `predict_proba()` function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d7879-b75a-4e54-8d06-84a950b0cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predTree = modTree.predict_proba(featuresTest_norm)\n",
    "predTree[0:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5202b19-54a1-4885-bb01-c431a9de86d4",
   "metadata": {},
   "source": [
    "For each prediction, the first number in the `[]`s is the probability of `Missed` and the second number is the probability of `Paid`. (Although only 0's and 1s display, these numbers are being rounded to the nearest whole number.) \n",
    "\n",
    "For the `roc_curve()` function, we only need the probability of `Paid`. We can do this by providing \"column criteria\"...we want the 2nd column and, because Python always starts counting at 0, that means we need column 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd1aad-3ac9-414f-915c-baa4c6332b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predTree[0:19, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a7df8-5791-41cb-9329-b3ee0289eb4d",
   "metadata": {},
   "source": [
    "For the \"row criteria\", we've been displaying rows 0 to 19...if you want all rows, you can just type a `:`, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d734d42-773c-4cd7-991a-c16053a38fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predTree[:, 1]\n",
    "# this will show a preview of the first 3 and the last 3 probabilities for Paid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68130d-be90-49d2-b481-338b14a80ac6",
   "metadata": {},
   "source": [
    "Now, we're ready to use the `roc_curve()` function. Note: Because the `outcomeTest` variable contains `Paid` and `Missed` labels instead of 0's and 1's, we need to include the `pos_label` parameter to tell the function which category equates to a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8001f0-1239-4da4-a811-add3052c3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tree, tpr_tree, threshold_tree = sklearn.metrics.roc_curve(outcomeTest, \n",
    "                                                          predTree[:, 1], \n",
    "                                                         pos_label = \"Paid\")\n",
    "\n",
    "fpr_tree   # an array of false positive rates at a range of thresholds\n",
    "tpr_tree   # an array of true positive rates at a range of thresholds\n",
    "threshold_tree  # an array of the thresholds used to calculate the rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d183e4-dccc-486d-b320-55b0874f0e83",
   "metadata": {},
   "source": [
    "Now, let's plot the curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955dd2a-feea-4b39-8148-f822e67db7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr_tree, tpr_tree, label = \"Decision Tree\") #plots the ROC curve\n",
    "plt.plot([0,1], [0,1])                                #plots a reference line\n",
    "plt.legend()                                          #displays the legend\n",
    "plt.xlabel(\"False Positive Rate -->\")                 #labels the x-axis\n",
    "plt.ylabel(\"True Positive Rate -->\")                  #labels the y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35e0c4-f697-4237-a8c2-6b348ca2b3ad",
   "metadata": {},
   "source": [
    "Now you try...get predicted probabilities for the Random Forest model (`modForest`) and then plot the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97c8a4-8bcf-40bd-93f6-a7c7502f8aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a24d346a-c685-4f03-b5c7-6812b5d86c4c",
   "metadata": {},
   "source": [
    "Now, copy the `plt.plot()` line of code that drew the Random Forest ROC curve and paste it into the code cell with the Decision Tree ROC curve. See what happens when you combine the code.\n",
    "\n",
    "Run the following code cell to get the predicted probabilities and ROC curve for the Neural Network model and then copy/paste the `plt.plot()` line of code into the code cell for the Decision Tree above in order to display all 3 curves on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbaafb5-82f6-4c2c-84bf-695e01483a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predNN = modNN.predict_proba(featuresTest_norm)\n",
    "fpr_NN, tpr_NN, threshold_NN = sklearn.metrics.roc_curve(outcomeTest,\n",
    "                                                     predNN[:, 1],\n",
    "                                                     pos_label = \"Paid\")\n",
    "\n",
    "plt.plot(fpr_NN, tpr_NN, label = \"Neural Network\")\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate -->\")\n",
    "plt.ylabel(\"True Positive Rate -->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021da616-49ee-4c8e-bc0c-fa597b509986",
   "metadata": {},
   "source": [
    "For support vector machines, predictions for whether or not a customer `Missed` or `Paid` is based on distance to the support vector plane and not based on probability. Therefore, we need to use `decision_function()` instead of `predict_proba()` to get predictions. This will return 1 number instead of 2, so we also don't need to include `[:, 1]` in the `roc_curve()` command. Other than that, the rest of the code is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029a893-66cb-419b-8183-57890e6b139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predSVM = modSVM.decision_function(featuresTest_norm)\n",
    "fpr_SVM, tpr_SVM, threshold_SVM = sklearn.metrics.roc_curve(outcomeTest,\n",
    "                                                       predSVM,\n",
    "                                                       pos_label = \"Paid\")\n",
    "\n",
    "plt.plot(fpr_SVM, tpr_SVM, label = \"Support Vector Machine\")\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate -->\")\n",
    "plt.ylabel(\"True Positive Rate -->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8539e-fa8b-4633-8e93-f3b09a5cc174",
   "metadata": {},
   "source": [
    "Based on a comparison of the ROC curves, which model seems to be doing a better job of balancing predictions across the different thresholds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b894e35-1adb-4d23-b19a-243d708859ed",
   "metadata": {},
   "source": [
    "## AUC (Area under the Curve)\n",
    "\n",
    "While it's nice to be able to visually compare the ROC curves, the area under the curve (AUC) is a very useful measure to interpret the curve. The AUC is the probability a model will correctly classify the customer. We can use the `auc()` function from `sklearn.metrics` to calculate it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be10218-8039-46dd-a9bc-260ffbf6eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_tree = sklearn.metrics.auc(fpr_tree, tpr_tree)\n",
    "auc_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ff9de-dc9b-45fa-b872-71425c468081",
   "metadata": {},
   "source": [
    "We would interpret this as follows: the probability the Decision Tree will correctly classify someone as either `Paid` or `Missed` is 60.1%.\n",
    "\n",
    "Now you try...calculate the AUCs for the remaining 3 models. Which model has the highest AUC? Does this make sense based on the ROC curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09d6fa-44fa-47f4-a544-e443f2b52228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b6b3ed6-b5ed-49ef-8a0a-5f0def3a1503",
   "metadata": {},
   "source": [
    "## Youden's J Statistic\n",
    "\n",
    "Youden's J statistic is an easy way to determine which threshold best balances the true vs. false positive rates. It's calculated as `True Positive Rate - False Positive Rate`.\n",
    "\n",
    "We can calculate the Youden's J statistic for every threshold used when calculating the `roc_curve()` and then use the corresponding threshold for the highest J statistic. \n",
    "\n",
    "Based on the highest AUC, let's find the optimal threshold for the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482a53f-f003-40fa-97ad-aaed2d5251ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Youden's J stat for each threshold\n",
    "Jstat_NN = tpr_NN - fpr_NN\n",
    "\n",
    "# locate the index location of the highest J stat\n",
    "import numpy as np\n",
    "optimal_index = np.argmax(Jstat_NN)\n",
    "\n",
    "# locate the threshold at that index location\n",
    "optimal_threshold = threshold_NN[optimal_index] \n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04460e20-3090-4949-91b5-a95287661791",
   "metadata": {},
   "source": [
    "This suggests that, if we use the neural network model, any prediction with a probability less than 0.747 should be classified as `Missed` and anything above that threshold should be classified as `Paid`.\n",
    "\n",
    "Here's how to get the new classification report based on this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf404b0d-afdb-4801-a73f-867783a94620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate predictions using the optimal threshold\n",
    "predNN_optimal = (predNN[:, 1] >= optimal_threshold)\n",
    "\n",
    "# convert the predicted True/False values to \"Paid\" or \"Missed\"\n",
    "predNN_optimal = np.array([\"Paid\" if pred == True else \"Missed\" for pred in predNN_optimal])\n",
    "\n",
    "# print the classification report\n",
    "print(sklearn.metrics.classification_report(outcomeTest, predNN_optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf5969-02be-4df2-90f3-97ca5eae8611",
   "metadata": {},
   "source": [
    "As a reminder, here's the original classification report using a 50% threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d93a0-cd57-4e51-8c68-0f8104e7bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predNN_original = modNN.predict(featuresTest_norm)\n",
    "print(sklearn.metrics.classification_report(outcomeTest, predNN_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6a52a-6a98-4147-b6a4-a19ff9092738",
   "metadata": {},
   "source": [
    "The overall accuracy of the model decreased from 82% to 77%. However, notice the F1 score for the `Missed` category increased from 44% to 51%. \n",
    "\n",
    "If it's more costly to the bank to misclassify customers who miss their payments, the increased F1 score on `Missed` customers may outweigh the decreased overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d130f-85f6-442f-a7e6-472746ea674f",
   "metadata": {},
   "source": [
    "# Another Example\n",
    "\n",
    "For practice, see if you can build Decision Tree, Random Forest, SVM, and Neural Network models using the `diabetes.csv` data set and then evaluate the models using ROC curves, AUC, and Youden's J statistic. For your models, use the variable `Outcome` as your outcome variable and all other variables as your features. (For the neural network model, use `hidden_layer_sizes = (40,40)` and include `max_iter = 1000` as a parameter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab817d86-f846-4371-8a0f-1a34e23e2ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
